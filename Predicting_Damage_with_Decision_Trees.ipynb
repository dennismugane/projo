{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Predicting Damage with Decision Trees ",
      "provenance": [],
      "authorship_tag": "ABX9TyOeHiogT/z1pC+JbVTQSShf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dennismugane/projo/blob/master/Predicting_Damage_with_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predicting Damage with Decision Trees"
      ],
      "metadata": {
        "id": "oIpc5ZlVfHWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing libraries\n",
        "import sqlite3\n",
        "import warnings\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from category_encoders import OrdinalEncoder\n",
        "from IPython.display import VimeoVideo\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n"
      ],
      "metadata": {
        "id": "pQTbyjzcfQpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Prepare Data**\n",
        "\n",
        "Import "
      ],
      "metadata": {
        "id": "ehh7j0Jdfiyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wrangle(db_path):\n",
        "    # Connect to database\n",
        "    conn = sqlite3.connect(db_path)\n",
        "\n",
        "    # Construct query\n",
        "    query = \"\"\"\n",
        "        SELECT distinct(i.building_id) AS b_id,\n",
        "           s.*,\n",
        "           d.damage_grade\n",
        "        FROM id_map AS i\n",
        "        JOIN building_structure AS s ON i.building_id = s.building_id\n",
        "        JOIN building_damage AS d ON i.building_id = d.building_id\n",
        "        WHERE district_id = 4\n",
        "    \"\"\"\n",
        "\n",
        "    # Read query results into DataFrame\n",
        "    df = pd.read_sql(query, conn, index_col=\"b_id\")\n",
        "\n",
        "    # Identify leaky columns\n",
        "    drop_cols = [col for col in df.columns if \"post_eq\" in col]\n",
        "\n",
        "    # Add high-cardinality / redundant column\n",
        "    drop_cols.append(\"building_id\")\n",
        "\n",
        "    # Create binary target column\n",
        "    df[\"damage_grade\"] = df[\"damage_grade\"].str[-1].astype(int)\n",
        "    df[\"severe_damage\"] = (df[\"damage_grade\"] > 3).astype(int)\n",
        "\n",
        "    # Drop old target\n",
        "    drop_cols.append(\"damage_grade\")\n",
        "\n",
        "    # Drop multicollinearity column\n",
        "    drop_cols.append(\"count_floors_pre_eq\")\n",
        "\n",
        "    # Drop columns\n",
        "    df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ChporEkcf4Y4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = wrangle(\"/home/jovyan/nepal.sqlite\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "R15f0cjOgK9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Split**\n",
        "\n",
        "Create your feature matrix X and target vector y. Your target is \"severe_damage\".\n"
      ],
      "metadata": {
        "id": "p3XRAQJOo2Tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target = \"severe_damage\"\n",
        "X = df.drop(columns = \"severe_damage\")\n",
        "y = df[target]"
      ],
      "metadata": {
        "id": "K4Kg0azDpAea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Divide your data (X and y) into training and test sets using a randomized train-test split.\n",
        "\n",
        " Your test set should be 20% of your total data. And set a random_state for reproducibility."
      ],
      "metadata": {
        "id": "8ZoDVEr_pSlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "Faa_bH2gpia4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Divide your training data (X_train and y_train) into training and validation sets using a randomized train-test split. \n",
        " \n",
        " Your validation data should be 20% of the remaining data. Set a random_state."
      ],
      "metadata": {
        "id": "fiOE4YORpyQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train, y_train, test_size = 0.2, random_state = 42\n",
        ")"
      ],
      "metadata": {
        "id": "2j_HMHItp8N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.Build Model**\n",
        "\n",
        "Baseline\n",
        "\n",
        "Calculate the baseline accuracy score for your model."
      ],
      "metadata": {
        "id": "tDP9J5idqG-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_baseline = y_train.value_counts(normalize = True).max()\n",
        "print(\"Baseline Accuracy:\", round(acc_baseline, 2))"
      ],
      "metadata": {
        "id": "fWpsL7hNqR1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Iterate** \n",
        "\n",
        "Create a pipeline named model that contains a OrdinalEncoder transformer and a DecisionTreeClassifier predictor.\n",
        "\n",
        " fit our model to the training data."
      ],
      "metadata": {
        "id": "n3pja0RNqjSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build Model\n",
        "model = make_pipeline(\n",
        "    OrdinalEncoder(),\n",
        "    DecisionTreeClassifier(max_depth=7,random_state = 42)\n",
        ")\n",
        "# Fit model to training data\n",
        "model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "kzMSn1Ncqk1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the training and validation accuracy scores for your models\n",
        "acc_train = accuracy_score(y_train, model.predict(X_train))\n",
        "acc_val = model.score(X_val,y_val) \n",
        "\n",
        "print(\"Training Accuracy:\", round(acc_train, 2))\n",
        "print(\"Validation Accuracy:\", round(acc_val, 2))"
      ],
      "metadata": {
        "id": "8wVsbUzUs6iL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the get_depth method on the DecisionTreeClassifier in our model to see how deep our tree grew during training."
      ],
      "metadata": {
        "id": "K_gK0WXVtQnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_depth = model.named_steps['decisiontreeclassifier'].get_depth()\n",
        "print(\"Tree Depth:\", tree_depth)"
      ],
      "metadata": {
        "id": "t433wwtstDiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a range of possible values for max_depth hyperparameter of our model's\n",
        "\n",
        " DecisionTreeClassifier. depth_hyperparams should range from 1 to 50 by steps of 2."
      ],
      "metadata": {
        "id": "JdfttO0Qtnqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "depth_hyperparams = range(1,50,2)"
      ],
      "metadata": {
        "id": "Yi-nc30htvu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create empty lists for training and validation accuracy scores\n",
        "training_acc = []\n",
        "validation_acc = []\n",
        "\n",
        "for d in depth_hyperparams:\n",
        "    # Create model with `max_depth` of `d`\n",
        "    test_model = make_pipeline(\n",
        "        OrdinalEncoder(),\n",
        "        DecisionTreeClassifier(max_depth = d, random_state = 42)\n",
        "    )\n",
        "    # Fit model to training data\n",
        "    test_model.fit(X_train, y_train)\n",
        "    # Calculate training accuracy score and append to `training_acc`\n",
        "    training_acc.append(test_model.score(X_train, y_train))\n",
        "    # Calculate validation accuracy score and append to `training_acc`\n",
        "    validation_acc.append(test_model.score(X_val,y_val))\n",
        "\n",
        "print(\"Training Accuracy Scores:\", training_acc[:3])\n",
        "print(\"Validation Accuracy Scores:\", validation_acc[:3])"
      ],
      "metadata": {
        "id": "J1mH-4gyt_g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a visualization with two lines. The first line should plot the \n",
        "\n",
        "training_acc values as a function of depth_hyperparams, and the second should \n",
        "\n",
        "plot validation_acc as a function of depth_hyperparams. x-axis will be \n",
        "\n",
        "labeled \"Max Depth\", and the y-axis \"Accuracy Score\". Also we include a legend so \n",
        "\n",
        "that our audience can distinguish between the two lines."
      ],
      "metadata": {
        "id": "xNLJ0WIQuN7S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot `depth_hyperparams`, `training_acc`\n",
        "plt.plot(depth_hyperparams, training_acc, label = 'training')\n",
        "plt.plot(depth_hyperparams, validation_acc, label = 'validation')\n",
        "plt.xlabel('max depth')\n",
        "plt.ylabel('accuracy score')\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "iDemY7OiuhjS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Evaluate**\n",
        "\n",
        " Based on our visualization,we choose the max_depth value that leads to the best validation accuracy score. Then retrain our original model with that max_depth value. Lastly, check how our tuned model performs on our test set by calculating the test accuracy score below. And see if we solved the overfitting issue we had initially."
      ],
      "metadata": {
        "id": "k9aFBmLJvqSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_acc = model.score(X_test, y_test)\n",
        "print(\"Test Accuracy:\", round(test_acc, 2))"
      ],
      "metadata": {
        "id": "2dXgiqP7wTSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Communicate**\n",
        "\n",
        "we plot the decision tree \n"
      ],
      "metadata": {
        "id": "F0iZ8lxgwcri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create larger figure\n",
        "fig, ax = plt.subplots(figsize=(25, 12))\n",
        "# Plot tree\n",
        "plot_tree(\n",
        "    decision_tree=model.named_steps['decisiontreeclassifier'],\n",
        "    feature_names=X_train.columns,\n",
        "    class_names = ['Not severe damaged', 'severe damaged'],\n",
        "    filled=True,  # Color leaf with class\n",
        "    rounded=True,  # Round leaf edges\n",
        "    proportion=True,  # Display proportion of classes in leaf\n",
        "    max_depth=3,  # Only display first 3 levels\n",
        "    fontsize=12,  # Enlarge font\n",
        "    ax=ax,  # Place in figure axis\n",
        ");"
      ],
      "metadata": {
        "id": "FbFENpozwqYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assign the feature names and importances of our model to the variables below. For the features, we can get them from the column names in \n",
        "our training set."
      ],
      "metadata": {
        "id": "JF6XmauVw5u3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = X_train.columns\n",
        "importances = model.named_steps['decisiontreeclassifier'].feature_importances_\n",
        "\n",
        "print(\"Features:\", features[:3])\n",
        "print(\"Importances:\", importances[:3])"
      ],
      "metadata": {
        "id": "5MODr7gnxDt7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a pandas Series named feat_imp, where the index is features and the values are our importances. The Series should be sorted from smallest to largest importance."
      ],
      "metadata": {
        "id": "v_sDw0HKxQdh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feat_imp = pd.Series(importances, index = features).sort_values()\n",
        "feat_imp.head()"
      ],
      "metadata": {
        "id": "b-VqQzwKxVza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create a horizontal bar chart with all the features in feat_imp. Labelling our x-axis \"Gini Importance\" and y-axis \"features\""
      ],
      "metadata": {
        "id": "rZm9HPRwxorm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create horizontal bar chart\n",
        "feat_imp.plot(kind = 'barh')\n",
        "plt.xlabel(\"Gini Importance\")\n",
        "plt.ylabel(\"features\");"
      ],
      "metadata": {
        "id": "O7JdrBfqx89t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}